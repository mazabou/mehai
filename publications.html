<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Mehdi Azabou - Publications</title>
    <meta name="keywords" content="Mehdi Azabou">
    <meta name="author" content="Mehdi Azabou">
    <link rel="shortcut icon" href="assets/üß†.ico"/>
    <script src="https://kit.fontawesome.com/3875b07657.js" crossorigin="anonymous"></script>
    <script src="modal.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://bulma.io/css/bulma-docs.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">

    <script src="https://cdn.jsdelivr.net/npm/tsparticles-confetti@2.9.3/tsparticles.confetti.bundle.min.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SMP0QT1S6Z"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-SMP0QT1S6Z', {
            'page_path': '/publications'
        });
    </script>
</head>
<body>

<div class="section is-max-desktop navbar" style="z-index: 0;">
    <nav class="breadcrumb is-large" aria-label="breadcrumbs">
        <ul>
            <li><a class="colorme" href="/">About</a></li>
            <li class="is-active"><a href="#" aria-current="page">Publications</a></li>
            <li><a class="colorme" href="resume">Resume</a></li>
            <li><a class="colorme" href="https://scholar.google.com/citations?user=jXxyYCoAAAAJ&hl=en" target="_blank"
                   rel="noopener noreferrer"> <i class="ai ai-google-scholar-square"></i></a></li>
            <li><a class="colorme" href="https://www.semanticscholar.org/author/Mehdi-Azabou/2039962468" target="_blank"
                   rel="noopener noreferrer"> <i class="ai ai-semantic-scholar"></i></a></li>
            <li><a class="colorme" href="https://github.com/mazabou" target="_blank" rel="noopener noreferrer"> <i
                    class="fa fa-github"></i></a></li>
            <li><a class="colorme" href="https://twitter.com/mehdiazabou" target="_blank" rel="noopener noreferrer"> <i
                    class="fa-brands fa-twitter"></i></a></li>
        </ul>
    </nav>
</div>

<section class="section" style="z-index: 2;">
    <div style="margin-bottom: 0.75rem;">
        <!--
        <div class="block container bd-post-content bottom_padding">
            <div class="dropdown is-hoverable">
                <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Tag</span>
                    </span>
                </div>
                <div class="dropdown-menu" role="menu">
                    <div class="dropdown-content">
                        <div class="dropdown-item">
                            <p class="has-text-weight-normal is-size-7">TL;DR</p>
                        </div>
                    </div>
                </div>
            </div>
            <span class="is-size-7"> <i class="fa-solid fa-angle-left"></i> hover to get TL;DR</span> &nbsp;&nbsp;
            <a><i class="fa-solid fa-ellipsis"></i></a> <span class="is-size-7"> <i class="fa-solid fa-angle-left"></i> click to get abstract.</span>
        </div>
        -->
        <div class="button_container bottom_padding">
            <span class="has-text-weight-bold top_padding">Filter: &nbsp;</span>
            <button class="tag-button tag is-info is-rounded is-light has-text-weight-bold is-medium top_padding" data-tag="ssl" data-color="hsl(206, 70%, 96%)" data-breadcrumbcolor="hsl(204, 71%, 39%)"> Self-Supervised Learning </button>&nbsp;
            <button class="tag-button tag is-success is-rounded is-light has-text-weight-bold is-medium top_padding" data-tag="neuro" data-color="hsl(142, 52%, 96%)" data-breadcrumbcolor="hsl(141, 53%, 31%)"> Neuroscience </button>&nbsp;
            <button class="tag-button tag is-danger is-rounded is-light has-text-weight-bold is-medium top_padding" data-tag="behavior" data-color="hsl(347, 90%, 96%)" data-breadcrumbcolor="hsl(348, 86%, 43%)">Behavior</button>&nbsp;
            <button class="tag-button tag is-warning is-rounded is-light has-text-weight-bold is-medium top_padding" data-tag="graphs" data-color="hsl(48, 100%, 96%)" data-breadcrumbcolor="hsl(48, 100%, 29%)">Graphs</button>&nbsp;
            <button class="tag-button tag is-rounded is-light has-text-weight-bold is-medium top_padding" data-tag="ot" data-color="hsl(0, 0%, 96%)" data-breadcrumbcolor="hsl(0, 0%, 30%)">Optimal Transport</button>&nbsp; &nbsp;
            <button class="delete top_padding" id="reset"></button>
        </div>
    </div>

    <div class="block container bd-post-content entry year" data-tags="header">
        <h2 class="title">
            <p class="is-size-66">
                2023
            </p>
        </h2>
    </div>

    <div class="wrapper">
    <div class="entry block container bd-post-content target-div highlighter" data-tags="graphs ssl">
        <article class="media bottom_padding">
            <figure class="is-hidden-mobile media-left">
                <p class="image is-128x128 ">
                    <img src="covers/halfhop.png" data-prompt="zooming into a graph/network with a magnifying glass. black background. high quality">
                </p>
            </figure>
            <div class="media-content">
                <div>
                    <div class="block container bd-post-content">
                        <h1 class="title">
                            <p class="is-size-55">
                                <i class="fa-solid fa-magnifying-glass"></i>
                                Half-Hop: A graph upsampling approach for slowing down message passing
                                <!--<a class="js-modal-trigger" data-target="forest-trans-modal"><i class="fa-solid fa-ellipsis"></i></a></span>-->
                            </p>
                            <p class="tagp">

                            <div class="dropdown is-hoverable">
                                <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-warning is-rounded is-light has-text-weight-bold">Geometric Deep Learning</span>
                    </span>
                </div>
                <div class="dropdown is-hoverable">
                    <div class="dropdown-trigger">
                <span class="ghostbutton">
                    <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                </span>
                    </div>
                    <div class="dropdown-menu" role="menu">
                        <div class="dropdown-content">
                            <div class="dropdown-item">
                                <p class="has-text-weight-normal is-size-7">Self-Supervised Learning</p>
                            </div>
                            <hr class="dropdown-divider">
                            <div class="dropdown-item">
                                <p class="has-text-weight-normal is-size-7">Half-Hop is used to generate more dirverse views for methods like BGRL and GRACE.</p>
                            </div>
                            <!--
                            <hr class="dropdown-divider">

                            -->
                        </div>
                    </div>
                </div>
                </div>

                            </p>
                        </h1>
                        <div class="subtitle subtitle-no-margin">
                            <p class="authors"><em><b>Mehdi Azabou</b>, Venkataramana Ganesh, Shantanu Thakoor, Chi-Heng Lin,
                                Lakshmi Sathidevi, Ran Liu, Michal Valko, Petar Veliƒçkoviƒá, Eva L. Dyer</em></p>
                            <p class="conferencetitle">Accepted at ICML 2023</p>
                            <!--
                            <span class="nowrap"><a href="https://nerdslab.github.io/EIT/" target="_blank" rel="noopener noreferrer"> <i class="fa fa-book"></i> Project page </a>&nbsp;</span>
                            <span class="nowrap"><a href="https://arxiv.org/pdf/2206.06131.pdf" target="_blank" rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                            <span class="nowrap"><a href="https://github.com/nerdslab/EIT" target="_blank" rel="noopener noreferrer"> <i class="fa fa-github"></i> Code </a>&nbsp;</span>
                            <span class="nowrap"><a href="https://arxiv.org/abs/2206.06131" target="_blank" rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                            <span class="nowrap"><a class="js-modal-trigger" data-target="forest-trans-cite"> <i class="fa-solid fa-quote-right"></i> Cite</a></span>
                            -->
                        </div>
                        <div id="halfhop-abstract" style="display:none;">
                            <b>Abstract:</b> Message passing neural networks have shown a lot of success on graph-structured data.
                            However, there are many instances where message passing can lead to over-smoothing or fail when neighboring nodes belong to different classes.
                            In this work, we introduce a simple yet general framework for improving learning in message passing neural networks.
                            Our approach essentially upsamples edges in the original graph by adding ``slow nodes'' at each
                            edge that can mediate communication between a source and a target node. Our method only modifies the input graph,
                            making it plug-and-play and easy to use with existing models. To understand the benefits of slowing down message passing,
                            we provide theoretical and empirical analyses. We report results on several supervised and self-supervised benchmarks,
                            and show improvements across the board, notably in heterophilic conditions where adjacent nodes may have different
                            labels. We also show how our method can be used to generate multi-scale views for graph self-supervised learning.
                        </div>
                    </div>
                </div>
            </div>
            <div class="media-right">
                <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="halfhop-abstract"></a>
            </div>
        </article>
    </div>
    <canvas id="my-canvas"></canvas>
    </div>



    <div id="halfhop-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">

                <div>
                    <b>Abstract:</b> Message passing neural networks have shown a lot of success on graph-structured data.
                    However, there are many instances where message passing can lead to over-smoothing or fail when neighboring nodes belong to different classes.
                    In this work, we introduce a simple yet general framework for improving learning in message passing neural networks.
                    Our approach essentially upsamples edges in the original graph by adding ``slow nodes'' at each
                    edge that can mediate communication between a source and a target node. Our method only modifies the input graph,
                    making it plug-and-play and easy to use with existing models. To understand the benefits of slowing down message passing,
                    we provide theoretical and empirical analyses. We report results on several supervised and self-supervised benchmarks,
                    and show improvements across the board, notably in heterophilic conditions where adjacent nodes may have different
                    labels. We also show how our method can be used to generate multi-scale views for graph self-supervised learning.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div class="block container bd-post-content entry" data-tags="neuro">
        <article class="media bottom_padding">
            <figure class="media-left is-hidden-mobile">
                <p class="image is-128x128">
                    <img src="covers/lolcat.png" data-prompt="neuron transcriptomic, firing, digital art, high resolution, orange, blue">
                </p>
            </figure>
            <div class="media-content">
                <div class="block container bd-post-content">
                    <h1 class="title">
                        <p class="is-size-55">
                            <i class="fa-solid fa-barcode"></i>
                            Transcriptomic cell type structures in vivo neuronal activity across multiple time <span
                                class="nowrap">scales
                    </span>
                        </p>
    
                        <p class="tagp">
                        <div class="dropdown is-hoverable">
                            <div class="dropdown-trigger">
                        <span class="ghostbutton">
                            <span class="tag is-success is-rounded is-light has-text-weight-bold">Neuroscience</span>
                        </span>
                            </div>
                            <div class="dropdown-menu" role="menu">
                                <div class="dropdown-content">
                                    <div class="dropdown-item">
                                        <p class="has-text-weight-normal is-size-7">We reveal the existence of
                                            characteristic <b>computational fingerprints</b> of transcriptomic cell types,
                                            during neuronal activity across diverse contexts: drifting gratings,
                                            naturalistic movies.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
    
                        <div class="dropdown  is-hoverable">
                            <div class="dropdown-trigger">
                        <span class="ghostbutton">
                            <span class="tag is-rounded is-light has-text-weight-bold">Attention</span>
                        </span>
                            </div>
                            <div class="dropdown-menu" role="menu">
                                <div class="dropdown-content">
                                    <div class="dropdown-item">
                                        <p class="has-text-weight-normal is-size-7">LOLCAT uses soft-attention, which
                                            enables the model to <b>aggregate trial information</b> across different scales.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>
    
                        <!-- <div class="dropdown is-hoverable">
                            <div class="dropdown-trigger">
                        <span class="ghostbutton">
                            <span class="tag is-rounded is-info is-light has-text-weight-bold">Preprint</span>
                        </span>
                            </div>
                        </div>
                        </p> -->
    
                    </h1>
                    <div class="subtitle subtitle-no-margin">
                        <p class="authors"><em>Aidan Schneider<sup>+</sup>, <b>Mehdi Azabou</b><sup>+</sup>, Louis
                            McDougall-Vigier, David Parks, Sahara Ensley, Kiran Bhaskaran-Nair,
                            Tom Nowakowski, Eva L. Dyer<sup>*</sup>, Keith B. Hengen<sup>*</sup></em></p>
                        <p><em><sup>+</sup>Contributed equally as co-first authors</em></p>
                        <p class="conferencetitle">Cell Reports, Volume 42, Issue 4, April 2023.</p>
                    </div>
                        <div id="lolcat-abstract" style="display:none;">
                            <!-- <img src="./assets/lolcat.jpg" alt=""> -->
                            <b>Abstract:</b> Cell type is hypothesized to be a key determinant of the role of a neuron within a
                            circuit. However, it is unknown whether a neuron‚Äôs transcriptomic type influences the timing of its
                            activity in the intact brain. In other words, can transcriptomic cell type be extracted from the
                            time series of a neuron‚Äôs activity? To address this question, we developed a new deep learning
                            architecture that learns features of interevent intervals across multiple timescales (milliseconds
                            to >30 min). We show that transcriptomic cell class information is robustly embedded in the timing
                            of single neuron activity recorded in the intact brain of behaving animals (calcium imaging and
                            extracellular electrophysiology), as well as in a bio-realistic model of visual cortex. In contrast,
                            we were unable to reliably extract cell identity from summary measures of rate, variance, and
                            interevent interval statistics. We applied our analyses to the question of whether transcriptomic
                            subtypes of excitatory neurons represent functionally distinct classes. In the calcium imaging
                            dataset, which contains a diverse set of excitatory Cre lines, we found that a subset of excitatory
                            cell types are computationally distinguishable based upon their Cre lines, and that excitatory types
                            can be classified with higher accuracy when considering their cortical layer and projection class.
                            Here we address the fundamental question of whether a neuron, within a complex cortical network,
                            embeds a fingerprint of its transcriptomic identity into its activity. Our results reveal robust
                            computational fingerprints for transcriptomic types and classes across diverse contexts, defined
                            over multiple timescales.

                    </div>
                    <div class="subtitle">
                        <hr>
                        <span class="nowrap"><a href="https://reader.elsevier.com/reader/sd/pii/S2211124723003297?token=C10DAF41EC33244826A08D09CE24930253CD6423459C834B965CD6BAE3253EBF6845ED9C2ADC6CE3EA68114690C2560E&originRegion=us-east-1&originCreation=20230330205233"
                                                target="_blank" rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a> &nbsp;</span>
                        <!-- <span class="nowrap"><a href="https://www.biorxiv.org/content/10.1101/2022.07.10.499487v1"
                                                target="_blank" rel="noopener noreferrer"> <i class="fa fa-desktop"></i> bioRxiv </a>&nbsp;</span> -->
                        <span class="nowrap"><a href="https://www.cell.com/cell-reports/fulltext/S2211-1247(23)00329-7"
                            target="_blank" rel="noopener noreferrer"> <i class="fa fa-desktop"></i> Cell Reports </a>&nbsp;</span>
                                                                        
                        <span class="nowrap"><a class="js-modal-trigger" data-target="lolcat-cite"> <i
                                class="fa-solid fa-quote-right"></i> Cite</a></span>
                    </div>
                </div>
            </div>
            <div class="media-right">
                <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="lolcat-abstract"></a>
            </div>
        </article>
        </div>
    
        <!-- > LOLCAT </!-->
    
    
        <div id="lolcat-modal" class="modal">
            <div class="modal-background"></div>
            <div class="modal-card">
                <header class="modal-card-head">
                    <p class="modal-card-title"></p>
                    <button class="delete" aria-label="close"></button>
                </header>
                <section class="modal-card-body">
                    <div class="image padded_block">
                        <img src="./assets/lolcat.jpg" alt="">
                    </div>
    
                    <div>
                        <b>Abstract:</b> Cell type is hypothesized to be a key determinant of the role of a neuron within a
                        circuit. However, it is unknown whether a neuron‚Äôs transcriptomic type influences the timing of its
                        activity in the intact brain. In other words, can transcriptomic cell type be extracted from the
                        time series of a neuron‚Äôs activity? To address this question, we developed a new deep learning
                        architecture that learns features of interevent intervals across multiple timescales (milliseconds
                        to >30 min). We show that transcriptomic cell class information is robustly embedded in the timing
                        of single neuron activity recorded in the intact brain of behaving animals (calcium imaging and
                        extracellular electrophysiology), as well as in a bio-realistic model of visual cortex. In contrast,
                        we were unable to reliably extract cell identity from summary measures of rate, variance, and
                        interevent interval statistics. We applied our analyses to the question of whether transcriptomic
                        subtypes of excitatory neurons represent functionally distinct classes. In the calcium imaging
                        dataset, which contains a diverse set of excitatory Cre lines, we found that a subset of excitatory
                        cell types are computationally distinguishable based upon their Cre lines, and that excitatory types
                        can be classified with higher accuracy when considering their cortical layer and projection class.
                        Here we address the fundamental question of whether a neuron, within a complex cortical network,
                        embeds a fingerprint of its transcriptomic identity into its activity. Our results reveal robust
                        computational fingerprints for transcriptomic types and classes across diverse contexts, defined
                        over multiple timescales.
                    </div>
                </section>
                <footer class="modal-card-foot"></footer>
            </div>
        </div>
    
        <div id="lolcat-cite" class="modal">
            <div class="modal-background"></div>
            <div class="modal-card">
                <header class="modal-card-head">
                    <p class="modal-card-title">Cite this paper</p>
                    <button class="delete" aria-label="close"></button>
                </header>
                <section class="modal-card-body">
                    BibTex
                    <pre><code>@article{SCHNEIDER2023112318,
title = {Transcriptomic cell type structures in¬†vivo neuronal activity across multiple timescales},
journal = {Cell Reports},
volume = {42},
number = {4},
pages = {112318},
year = {2023},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2023.112318},
url = {https://www.sciencedirect.com/science/article/pii/S2211124723003297},
author = {Aidan Schneider and Mehdi Azabou and Louis McDougall-Vigier and David F. Parks and Sahara Ensley and Kiran Bhaskaran-Nair and Tomasz Nowakowski and Eva L. Dyer and Keith B. Hengen},
}
</code></pre>
                    APA
                    <pre><code>Schneider, A., Azabou, M., McDougall-Vigier, L., Parks, D. B., Ensley, S., Bhaskaran-Nair, K., Nowakowski, T., Dyer, E. L. & Hengen, K. B. (2023). Transcriptomic cell type structures in vivo neuronal activity across multiple time scales. <i>Cell Reports, Volume 42, Issue 4, 2023</i></code></pre>
                </section>
                <footer class="modal-card-foot"></footer>
            </div>
        </div>


    <div class="entry block container bd-post-content" data-tags="behavior,ssl,ot">
    <article class="media bottom_padding">
        <figure class="is-hidden-mobile media-left">
            <p class="image is-128x128 ">
                <img src="covers/bams-hoa.png" data-prompt="multiverse, a single branch that branches out to increasingly more branches, digital art, colorful">
            </p>
        </figure>
        <div class="media-content">
            <div>
                <div class="block container bd-post-content">
                    <h1 class="title">
                        <p class="is-size-55">
                            <i class="fa-solid fa-arrows-split-up-and-left"></i>
                            Relax, it doesn't matter how you get there: A new self-supervised approach for
                            multi-timescale behavior analysis
                            <!--<a class="js-modal-trigger" data-target="forest-trans-modal"><i class="fa-solid fa-ellipsis"></i></a></span>-->
                        </p>
                        <p class="tagp">

                        <div class="dropdown is-hoverable">
                            <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-danger is-rounded is-light has-text-weight-bold">Behavior</span>
                    </span>
                            </div>

                        </div>


                        <div class="dropdown is-hoverable">
                            <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                    </span>
                            </div>

                        </div>

                        <div class="dropdown is-hoverable">
                            <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Optimal Transport</span>
                    </span>
                            </div>
                        </div>
                        </p>
                    </h1>
                    <div class="subtitle subtitle-no-margin">
                        <p class="authors"><em><b>Mehdi Azabou</b>, Michael Mendelson, Nauman Ahad, Maks Sorokin,
                            Shantanu Thakoor, Carolina Urzay, Eva L. Dyer</em></p>
                        <p class="conferencetitle">Preprint, Mar 2023</p>
                    </div>
                    <div id="bams-hoa-abstract" style="display:none;">
                        <b>Abstract:</b> Natural behavior consists of dynamics that are complex
                        and unpredictable, especially when trying to predict many
                        steps into the future. While some success has been found
                        in building representations of behavior under constrained
                        or simplified task-based conditions, many of these models
                        cannot be applied to free and naturalistic settings where
                        behavior becomes increasingly hard to model. In this work,
                        we develop a multi-task representation learning model for
                        behavior that combines two novel components: (i) An action
                        prediction objecti√•ve that aims to predict the distribution of
                        actions over future timesteps, and (ii) A multi-scale architecture that builds separate latent spaces to accommodate
                        short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and
                        global dynamics in realistic robots in varying environments
                        and terrains, we apply our method to the MABe 2022 Multi-agent behavior challenge, where our model ranks 1st overall
                        and on all global tasks, and 1st on 4 out of 9 frame-level
                        subtasks. In all of these cases, we show that our model can
                        build representations that capture the many different factors
                        that drive behavior and solve a wide range of downstream
                        tasks.
                    </div>
                    <div class="subtitle">
                        <hr>
                        <span class="nowrap"><a href="https://multiscale-behavior.github.io/" target="_blank" rel="noopener noreferrer"> <i class="fa fa-book"></i> Project page </a>&nbsp;</span>
                        <span class="nowrap"><a href="https://arxiv.org/pdf/2303.08811.pdf" target="_blank" rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                        <span class="nowrap"><a href="https://arxiv.org/pdf/2303.08811" target="_blank" rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                        <span class="nowrap"><a class="js-modal-trigger" data-target="bams-hoa-cite"> <i class="fa-solid fa-quote-right"></i> Cite</a></span>
                        <!--
                        <span class="nowrap"><a href="https://github.com/nerdslab/EIT" target="_blank" rel="noopener noreferrer"> <i class="fa fa-github"></i> Code </a>&nbsp;</span>
                        -->
                    </div>
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="bams-hoa-abstract"></a>
        </div>
    </article>
    </div>


    <div id="bams-hoa-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">

                <div>
                    <b>Abstract:</b> Natural behavior consists of dynamics that are complex
                    and unpredictable, especially when trying to predict many
                    steps into the future. While some success has been found
                    in building representations of behavior under constrained
                    or simplified task-based conditions, many of these models
                    cannot be applied to free and naturalistic settings where
                    behavior becomes increasingly hard to model. In this work,
                    we develop a multi-task representation learning model for
                    behavior that combines two novel components: (i) An action
                    prediction objecti√•ve that aims to predict the distribution of
                    actions over future timesteps, and (ii) A multi-scale architecture that builds separate latent spaces to accommodate
                    short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and
                    global dynamics in realistic robots in varying environments
                    and terrains, we apply our method to the MABe 2022 Multi-agent behavior challenge, where our model ranks 1st overall
                    and on all global tasks, and 1st on 4 out of 9 frame-level
                    subtasks. In all of these cases, we show that our model can
                    build representations that capture the many different factors
                    that drive behavior and solve a wide range of downstream
                    tasks.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="bams-hoa-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@misc{azabou2023relax,
doi = {10.48550/ARXIV.2303.08811},
url = {https://arxiv.org/abs/2303.08811},
author = {Azabou, Mehdi and Mendelson, Michael and Ahad, Nauman and Sorokin, Maks and Thakoor, Shantanu and Urzay, Carolina and Dyer, Eva L.},
title = {Relax, it doesn't matter how you get there: A new self-supervised approach for multi-timescale behavior analysis},
publisher = {arXiv},
year = {2023}
}</code></pre>
                APA
                <pre><code>Mendelson, M., Azabou, M., Jacob, S., Grissom, N., Darrow, D., Ebitz, B., Herman, A. & Dyer, E. L. (2023). Learning signatures of decision making from many individuals playing the same game. <i>arXiv.</i></code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>


    <div class="block container bd-post-content entry" data-tags="behavior,ssl">
    <article class="media bottom_padding">
        <figure class="is-hidden-mobile media-left">
            <p class="image is-128x128">
                <img src="covers/bams-human.png" data-prompt="a lot of slot machines in the shape of robots stacked next to each other, high resolution, isometric view, 3d render">
            </p>
        </figure>
        <div class="media-content">
            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-users"></i>
                        Learning signatures of decision making from many individuals playing the same game
                        <!--<a class="js-modal-trigger" data-target="forest-trans-modal"><i class="fa-solid fa-ellipsis"></i></a></span>-->
                    </p>
                    <p class="tagp">

                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-danger is-rounded is-light has-text-weight-bold">Behavior</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We study a large-scale behavioral dataset
                                        from <b>1,000 humans</b> playing a <b>3-armed bandit task</b>. By learning to predict future choices,
                                        we identify characteristic playing <b>strategies</b> and signatures of differences across individuals.</p>
                                </div>
                                <!--
                                <hr class="dropdown-divider">

                                -->
                            </div>
                        </div>
                    </div>


                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Self-Supervised Learning</p>
                                </div>
                                <hr class="dropdown-divider">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">To simultaneously extract both <b>global</b>
                                        and <b>local</b> variables from complex human behavior, our method combines a
                                        multi-scale temporal convolutional network with latent prediction tasks,
                                        where we encourage embeddings across the entire sequence, as well as subsets
                                        of the sequence, to be mapped to similar points in the latent space.</p>
                                </div>
                                <!--
                                <hr class="dropdown-divider">

                                -->
                            </div>
                        </div>
                    </div>
                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em>Michael Mendelson<sup>+</sup>, <b>Mehdi Azabou</b><sup>+</sup>, Suma Jacob, Nicola Grissom, David P. Barrow,
                        Becket Ebitz, Alexander Herman, Eva L. Dyer</em></p>
                    <p><em><sup>+</sup>Contributed equally as co-first authors</em></p>
                    <p class="conferencetitle">To appear at the 11th International IEEE EMBS Conference on Neural Engineering
                        (NER'23), Baltimore, Maryland, <span class="nowrap">April 2023</span></p>
                </div>

                <div id="human-abstract" style="display: none;">
                    <b>Abstract:</b> Human behavior is incredibly complex and the factors that drive decision making‚Äîfrom
                    instinct, to strategy, to biases between individuals‚Äîoften vary over multiple timescales.
                    In this paper, we design a predictive framework that learns representations to encode an individual's
                    `behavioral style', i.e. long-term behavioral trends, while simultaneously predicting future actions
                    and choices. The model  explicitly separates representations into three latent spaces: the recent
                    past space, the short-term space, and the long-term space where we hope to capture individual
                    differences. To simultaneously extract both global and local variables from complex human behavior,
                    our method combines a multi-scale temporal convolutional network with latent prediction tasks,
                    where we encourage embeddings across the entire sequence, as well as subsets of the sequence,
                    to be mapped to similar points in the latent space.

                    We develop and apply our method to a large-scale behavioral dataset from 1,000 humans playing a
                    3-armed bandit task, and analyze what our model's resulting embeddings reveal about the human
                    decision making process. In addition to predicting future choices, we show that our model can learn
                    rich representations of human behavior over multiple timescales and provide signatures of
                    differences in individuals.
                </div>
                <div class="subtitle">
                    <hr>
                    <span class="nowrap"><a href="https://arxiv.org/pdf/2302.11023.pdf" target="_blank" rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/abs/2302.11023" target="_blank" rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="human-bandit-cite"> <i class="fa-solid fa-quote-right"></i> Cite</a></span>
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="human-abstract"></a>
        </div>
    </article>
    </div>


    <div id="human-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">

                <div>
                    <b>Abstract:</b> Human behavior is incredibly complex and the factors that drive decision making‚Äîfrom
                    instinct, to strategy, to biases between individuals‚Äîoften vary over multiple timescales.
                    In this paper, we design a predictive framework that learns representations to encode an individual's
                    `behavioral style', i.e. long-term behavioral trends, while simultaneously predicting future actions
                    and choices. The model  explicitly separates representations into three latent spaces: the recent
                    past space, the short-term space, and the long-term space where we hope to capture individual
                    differences. To simultaneously extract both global and local variables from complex human behavior,
                    our method combines a multi-scale temporal convolutional network with latent prediction tasks,
                    where we encourage embeddings across the entire sequence, as well as subsets of the sequence,
                    to be mapped to similar points in the latent space.

                    We develop and apply our method to a large-scale behavioral dataset from 1,000 humans playing a
                    3-armed bandit task, and analyze what our model's resulting embeddings reveal about the human
                    decision making process. In addition to predicting future choices, we show that our model can learn
                    rich representations of human behavior over multiple timescales and provide signatures of
                    differences in individuals.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>


    <div id="human-bandit-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@misc{mendelson2023,
  doi = {10.48550/ARXIV.2302.11023},
  url = {https://arxiv.org/abs/2302.11023},
  author = {Mendelson, Michael J and Azabou, Mehdi and Jacob, Suma and Grissom, Nicola and Darrow, David and Ebitz, Becket and Herman, Alexander and Dyer, Eva L.},
  title = {Learning signatures of decision making from many individuals playing the same game},
  publisher = {arXiv},
  year = {2023},
}</code></pre>
                APA
                <pre><code>Mendelson, M., Azabou, M., Jacob, S., Grissom, N., Darrow, D., Ebitz, B., Herman, A. & Dyer, E. L. (2023). Learning signatures of decision making from many individuals playing the same game. <i>arXiv.</i></code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div class="block container bd-post-content entry" data-tags="neuro">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/changepoint.png" data-prompt="population of neurons morphing from one color to the other from left to right with a clear boundary, digital art">
            </p>
        </figure>
        <div class="media-content">

            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-road-spikes"></i>
                        Detecting change points in neural population activity with contrastive metric learning
                        <!--<a class="js-modal-trigger" data-target="forest-trans-modal"><i class="fa-solid fa-ellipsis"></i></a></span>-->
                    </p>
                    <p class="tagp">
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-success is-rounded is-light has-text-weight-bold">Neuroscience</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">
                                        We introduce an approach for identifying <b>switches</b> in neural population during
                                        <b>free behavior</b>, and find that different groups of neurons are useful for different contexts. </p>
                                </div>
                            </div>
                        </div>
                    </div>
                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em>Carolina Urzay, Nauman Ahad, <b>Mehdi Azabou</b>, Aidan Schneider, Geethika Atamkuri,
                        Keith B. Hengen, Eva L. Dyer</em></p>
                    <p class="conferencetitle">To appear at the 11th International IEEE EMBS Conference on Neural Engineering
                        (NER'23), Baltimore, Maryland, <span class="nowrap">April 2023</span></p>
                    <!--
                    <span class="nowrap"><a href="https://nerdslab.github.io/EIT/" target="_blank" rel="noopener noreferrer"> <i class="fa fa-book"></i> Project page </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/pdf/2206.06131.pdf" target="_blank" rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://github.com/nerdslab/EIT" target="_blank" rel="noopener noreferrer"> <i class="fa fa-github"></i> Code </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/abs/2206.06131" target="_blank" rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="forest-trans-cite"> <i class="fa-solid fa-quote-right"></i> Cite</a></span>
                    -->
                </div>
                <div id="urzay-22-abstract" style="display:none;">
                    <b>Abstract:</b> Finding points in time where the distribution of neural responses changes (change
                    points) is an important step in many neural data analysis pipelines. However, in complex and free
                    behaviors, where we see different types of shifts occurring at different rates, it can be difficult to
                    use existing methods for change point (CP) detection because they can't necessarily handle different
                    types of changes that may occur in the underlying neural distribution. In this work, we introduce a new
                    approach for finding changes in neural population states across diverse activities and arousal states
                    occurring in free behavior. Our model follows a contrastive learning approach: we learn a metric for CP
                    detection based on maximizing the Sinkhorn divergences of neuron firing rates across two sides of a
                    labeled CP. We apply this method to a 12-hour neural recording of a freely behaving mouse to detect
                    changes in sleep stages and behavior. We show that when we learn a metric, we can better detect change
                    points and also yield insights into which neurons and sub-groups are important for detecting certain
                    types of switches that occur in the brain.
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="urzay-22-abstract"></a>
        </div>
    </article>
    </div>

    <div class="block container bd-post-content entry year" data-tags="header">
        <h2 class="title">
            <p class="is-size-66">
                2022
            </p>
        </h2>
    </div>


    <!-- > Transformer </!-->
    <div class="block container bd-post-content entry" data-tags="neuro,transformer">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/eit.png" data-prompt="a forest of neurons, all converging to a single point in the sky, digital art">
            </p>
        </figure>
        <div class="media-content">

            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-tree"></i>
                        Seeing the forest and the tree: Building representations of both individual and collective dynamics with
                        <span class="nowrap">transformers
                </span>
                    </p>
                    <p class="tagp">
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-success is-rounded is-light has-text-weight-bold">Neuroscience</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Our novel transformer model performs
                                        competitively on <b>behavior decoding</b> tasks for spiking data, and shows promising
                                        potential for <b>across-session</b> and <b>across-subject</b> generalization.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Transformer</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We present a novel transformer architecture that
                                        first operates on individual time-series and then captures the interactions between them
                                        at a later stage; this induces a <b>permutation-invariance property</b> and can be used
                                        to transfer across systems of different size and order. </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em>Ran Liu, <b>Mehdi Azabou</b>, Max Dabagia, Jingyun Xiao, Eva L. Dyer</em></p>
                    <p class="conferencetitle">Neural Information Processing Systems (NeurIPS), 2022</p>
                    </div>
                    <div id="forest-trans-abstract" style="display: none;">
                        <!-- <img src="./assets/trans-forest.jpg" alt=""> -->
                        <b>Abstract:</b> Complex time-varying systems are often studied by abstracting away from the
                        dynamics of individual components to build a model of the population-level dynamics from the start.
                        However, when building a population-level description, it can be easy to lose sight of each
                        individual and how each contributes to the larger picture. In this paper, we present a novel
                        transformer architecture for learning from time-varying data that builds descriptions of both the
                        individual as well as the collective population dynamics. Rather than combining all of our data into
                        our model at the onset, we develop a separable architecture that operates on individual time-series
                        first before passing them forward; this induces a permutation-invariance property and can be used to
                        transfer across systems of different size and order. After demonstrating that our model can be
                        applied to successfully recover complex interactions and dynamics in many-body systems, we apply our
                        approach to populations of neurons in the nervous system. On neural activity datasets, we show that
                        our multi-scale transformer not only yields robust decoding performance, but also provides
                        impressive performance in transfer. Our results show that it is possible to learn from neurons in
                        one animal's brain and transfer the model on neurons in a different animal's brain, with
                        interpretable neuron correspondence across sets and animals. This finding opens up a new path to
                        decode from and represent large collections of neurons.
                    </div>
                <div class="subtitle">
                    <hr>
                    <span class="nowrap"><a href="https://nerdslab.github.io/EIT/" target="_blank" rel="noopener noreferrer"> <i
                            class="fa fa-book"></i> Project page </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/pdf/2206.06131.pdf" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://github.com/nerdslab/EIT" target="_blank" rel="noopener noreferrer"> <i
                            class="fa fa-github"></i> Code </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://openreview.net/forum?id=5aZ8umizItU" target="_blank"
                                            rel="noopener noreferrer"> <i
                            class="fa fa-desktop"></i> OpenReview </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/abs/2206.06131" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="forest-trans-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="forest-trans-abstract"></a>
        </div>
    </article>
    </div>



    <div id="forest-trans-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div class="image padded_block">
                    <img src="./assets/trans-forest.jpg" alt="">
                </div>
                <div>
                    <b>Abstract:</b> Complex time-varying systems are often studied by abstracting away from the
                    dynamics of individual components to build a model of the population-level dynamics from the start.
                    However, when building a population-level description, it can be easy to lose sight of each
                    individual and how each contributes to the larger picture. In this paper, we present a novel
                    transformer architecture for learning from time-varying data that builds descriptions of both the
                    individual as well as the collective population dynamics. Rather than combining all of our data into
                    our model at the onset, we develop a separable architecture that operates on individual time-series
                    first before passing them forward; this induces a permutation-invariance property and can be used to
                    transfer across systems of different size and order. After demonstrating that our model can be
                    applied to successfully recover complex interactions and dynamics in many-body systems, we apply our
                    approach to populations of neurons in the nervous system. On neural activity datasets, we show that
                    our multi-scale transformer not only yields robust decoding performance, but also provides
                    impressive performance in transfer. Our results show that it is possible to learn from neurons in
                    one animal's brain and transfer the model on neurons in a different animal's brain, with
                    interpretable neuron correspondence across sets and animals. This finding opens up a new path to
                    decode from and represent large collections of neurons.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="forest-trans-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@misc{https://doi.org/10.48550/arxiv.2206.06131,
  doi = {10.48550/ARXIV.2206.06131},
  url = {https://arxiv.org/abs/2206.06131},
  author = {Liu, Ran and Azabou, Mehdi and Dabagia, Max and Xiao, Jingyun and Dyer, Eva L.},
  keywords = {Neurons and Cognition (q-bio.NC), Machine Learning (cs.LG), FOS: Biological sciences, FOS: Biological sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}</code></pre>
                APA
                <pre><code>Liu, R., Azabou, M., Dabagia, M., Xiao, J., & Dyer, E. L. (2022). Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers. <i>arXiv preprint</i> arXiv:2206.06131.</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>


    <div class="block container bd-post-content entry" data-tags="graphs,ssl">
        <article class="media bottom_padding">
            <figure class="media-left is-hidden-mobile">
                <p class="image is-128x128">
                    <img src="covers/bgrl.png" data-prompt="two graphs colliding, 3d render, network, high resolution, black background, colorful, explosion">
                </p>
            </figure>
            <div class="media-content">
    
                <div class="block container bd-post-content">
                    <h1 class="title">
                        <p class="is-size-55">
                            <i class="fa-solid fa-circle-nodes"></i>
                            Large-scale representation learning on graphs via <span class="nowrap">bootstrapping
                    </span>
                        </p>
                        <p class="tagp">
                        <div class="dropdown is-hoverable">
                            <div class="dropdown-trigger">
                        <span class="ghostbutton">
                            <span class="tag is-light is-rounded is-warning is-light has-text-weight-bold">Geometric Deep Learning</span>
                        </span>
                            </div>
                            <div class="dropdown-menu" role="menu">
                                <div class="dropdown-content">
                                    <div class="dropdown-item">
                                        <p class="has-text-weight-normal is-size-7"> We train BGRL on several established node-level
                                            classification benchmarks, and show competitive results compared to supervised and
                                            self-supervised methods, while achieving a <b>2-10x reduction in memory costs</b> compared to
                                            GRACE, which enable us to <b>scale to large graphs</b>.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
    
                        <div class="dropdown  is-hoverable">
                            <div class="dropdown-trigger">
                        <span class="ghostbutton">
                            <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                        </span>
                            </div>
                            <div class="dropdown-menu" role="menu">
                                <div class="dropdown-content">
                                    <div class="dropdown-item">
                                        <p class="has-text-weight-normal is-size-7">Self-Supervised Learning</p>
                                    </div>
                                    <hr class="dropdown-divider">
                                    <div class="dropdown-item">
                                        <p class="has-text-weight-normal is-size-7">When trained with supervised data alone, GNNs
                                            can easily overfit and may fail to generalize. Using <b>DropEdge</b> and <b>DropFeat</b>
                                            augmentations, BGRL learns unsupervised graph representations.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                        </p>
    
                    </h1>
                    <div class="subtitle subtitle-no-margin">
                        <p class="authors"><em>Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, <b>Mehdi Azabou</b>,
                            Eva L. Dyer, Remi Munos, Petar Veliƒçkoviƒá, Michal Valko</em></p>
                        <p class="conferencetitle">International Conference on Learning Representations (ICLR), 2022.</p>
                        </div>
                        <div id="bgrl-abstract" style="display: none;">
                            <b>Abstract:</b> Self-supervised learning provides a promising path towards eliminating the need for
                            costly label information in representation learning on graphs. However, to achieve state-of-the-art
                            performance, methods often need large numbers of negative examples and rely on complex
                            augmentations. This can be prohibitively expensive, especially for large graphs. To address these
                            challenges, we introduce Bootstrapped Graph Latents (BGRL) - a graph representation learning method
                            that learns by predicting alternative augmentations of the input. BGRL uses only simple
                            augmentations and alleviates the need for contrasting with negative examples, and is thus scalable
                            by design. BGRL outperforms or matches prior methods on several established benchmarks, while
                            achieving a 2-10x reduction in memory costs. Furthermore, we show that BGRL can be scaled up to
                            extremely large graphs with hundreds of millions of nodes in the semi-supervised regime - achieving
                            state-of-the-art performance and improving over supervised baselines where representations are
                            shaped only through label information. In particular, our solution centered on BGRL constituted one
                            of the winning entries to the Open Graph Benchmark - Large Scale Challenge at KDD Cup 2021, on a
                            graph orders of magnitudes larger than all previously available benchmarks, thus demonstrating the
                            scalability and effectiveness of our approach.
                        </div>
                        <div class="subtitle">
                        <hr>
                        <span class="nowrap"><a href="https://openreview.net/pdf?id=0UXT6PpRpW" target="_blank"
                                                rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a> &nbsp;</span>
                        <span class="nowrap"><a href="https://github.com/nerdslab/bgrl" target="_blank"
                                                rel="noopener noreferrer"> <i class="fa fa-github"></i> Code </a> &nbsp;</span>
                        <span class="nowrap"><a href="https://iclr.cc/media/iclr-2022/Slides/6390.pdf" target="_blank"
                                                rel="noopener noreferrer"> <i class="fa-solid fa-person-chalkboard"></i> Slides </a> &nbsp;</span>
                        <span class="nowrap"><a href="https://openreview.net/forum?id=0UXT6PpRpW" target="_blank"
                                                rel="noopener noreferrer"> <i
                                class="fa fa-desktop"></i> OpenReview </a> &nbsp;</span>
                        <span class="nowrap"><a href="https://arxiv.org/abs/2102.06514" target="_blank"
                                                rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv</a>&nbsp;</span>
                        <span class="nowrap"><a class="js-modal-trigger" data-target="bgrl-cite"> <i
                                class="fa-solid fa-quote-right"></i> Cite</a></span>
                    </div>
                </div>
    
            </div>
            <div class="media-right">
                <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="bgrl-abstract"></a>
            </div>
        </article>
        </div>
    
        <div id="bgrl-modal" class="modal">
            <div class="modal-background"></div>
            <div class="modal-card">
                <header class="modal-card-head">
                    <p class="modal-card-title"></p>
                    <button class="delete" aria-label="close"></button>
                </header>
                <section class="modal-card-body">
                    <div class="image padded_block">
                        <img src="./assets/bgrl_arch.png" alt="">
                    </div>
                    <div>
                        <b>Abstract:</b> Self-supervised learning provides a promising path towards eliminating the need for
                        costly label information in representation learning on graphs. However, to achieve state-of-the-art
                        performance, methods often need large numbers of negative examples and rely on complex
                        augmentations. This can be prohibitively expensive, especially for large graphs. To address these
                        challenges, we introduce Bootstrapped Graph Latents (BGRL) - a graph representation learning method
                        that learns by predicting alternative augmentations of the input. BGRL uses only simple
                        augmentations and alleviates the need for contrasting with negative examples, and is thus scalable
                        by design. BGRL outperforms or matches prior methods on several established benchmarks, while
                        achieving a 2-10x reduction in memory costs. Furthermore, we show that BGRL can be scaled up to
                        extremely large graphs with hundreds of millions of nodes in the semi-supervised regime - achieving
                        state-of-the-art performance and improving over supervised baselines where representations are
                        shaped only through label information. In particular, our solution centered on BGRL constituted one
                        of the winning entries to the Open Graph Benchmark - Large Scale Challenge at KDD Cup 2021, on a
                        graph orders of magnitudes larger than all previously available benchmarks, thus demonstrating the
                        scalability and effectiveness of our approach.
                    </div>
                </section>
                <footer class="modal-card-foot"></footer>
            </div>
        </div>
    
        <div id="bgrl-cite" class="modal">
            <div class="modal-background"></div>
            <div class="modal-card">
                <header class="modal-card-head">
                    <p class="modal-card-title">Cite this paper</p>
                    <button class="delete" aria-label="close"></button>
                </header>
                <section class="modal-card-body">
                    BibTex
                    <pre><code>@inproceedings{
    thakoor2022largescale,
    title={Large-Scale Representation Learning on Graphs via Bootstrapping},
    author={Shantanu Thakoor and Corentin Tallec and Mohammad Gheshlaghi Azar and Mehdi Azabou and Eva L Dyer and Remi Munos and Petar Veli{\v{c}}kovi{\'c} and Michal Valko},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=0UXT6PpRpW}
    }</code></pre>
                    APA
                    <pre><code>Thakoor, S., Tallec, C., Azar, M. G., Azabou, M., Dyer, E. L., Munos, R., Veli{\v{c}}kovi{\'c}, P., & Valko, M. (2021). Large-scale representation learning on graphs via bootstrapping. <i>International Conference on Learning Representations</i>.</code></pre>
                </section>
                <footer class="modal-card-foot"></footer>
            </div>
        </div>
    

    <div class="block container bd-post-content entry" data-tags="neuro, neuroimaging">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/mtneuro.png" data-prompt="brain under the microscope, a neuron is visible through the lens of the microscope, digital art">
            </p>
        </figure>
        <div class="media-content">
            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-microscope"></i>
                        MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of <span
                            class="nowrap">Abstraction
                </span>
                    </p>

                    <p class="tagp">

                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Neuroimaging</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We introduce a multi-task neuroimaging benchmark
                                        for micrometer-resolution <b>X-ray microtomography imaging</b> of a large
                                        thalamocortical section of mouse brain. </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Vision</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We benchmark a series of 2D and 3D image <b>classfication</b> and <b>segmentation</b> methods.</p>
                                </div>
                            </div>
                        </div>
                    </div>


                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em>Jorge Quesada, Lakshmi Sathidevi, Ran Liu, Nauman Ahad, Joy M Jackson, <b>Mehdi
                        Azabou</b>, Jingyun Xiao, Chris Liding, Carolina Urzay, William Gray-Roncal, Erik Christopher Johnson,
                        Eva L. Dyer</em></p>
                    <p class="conferencetitle">Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track,
                        2022</p>
                    </div>
                    <div id="quesada-22-abstract" style="display: none;">
                        <b>Abstract:</b> There are multiple scales of abstraction from which we can describe the same image,
                        depending on whether we are focusing on fine-grained details or a more global attribute of the
                        image. In brain mapping, learning to automatically parse images to build representations of both
                        small-scale features (e.g., the presence of cells or blood vessels) and global properties of an
                        image (e.g., source brain region) is a crucial and open challenge. However, most existing datasets
                        and benchmarks for neuroanatomy consider only a single downstream task at a time. We introduce a new
                        dataset, annotations, and multiple downstream tasks that provide diverse ways to readout information
                        about brain structure and architecture from the same image. Our multi-task neuroimaging benchmark
                        (MTNeuro) is built on volumetric, micrometer-resolution X-ray microtomography imaging of a large
                        thalamocortical section of mouse brain, encompassing multiple cortical and subcortical regions, that
                        reveals dense reconstructions of the underlying microstructure (i.e., cell bodies, vasculature, and
                        axons). We generated a number of different prediction challenges and evaluated several supervised
                        and self-supervised models for brain-region prediction and pixel-level semantic segmentation of
                        microstructures. Our experiments not only highlight the rich heterogeneity of this dataset, but also
                        provide insights into how self-supervised approaches can be used to learn representations that
                        capture multiple attributes of a single image and perform well on a variety of downstream tasks.
                        Datasets, code, and pre-trained baseline models are provided at: https://mtneuro.github.io/.
                    </div>
                    <div class="subtitle">
                        <hr>
                    <span class="nowrap"><a href="https://mtneuro.github.io" target="_blank" rel="noopener noreferrer"> <i
                            class="fa-solid fa-globe"></i> Website  </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://openreview.net/pdf?id=5xuowSQ17vy" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://github.com/MTNeuro/MTNeuro" target="_blank" rel="noopener noreferrer"> <i
                            class="fa fa-github"></i> Code </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://openreview.net/forum?id=5xuowSQ17vy" target="_blank"
                                            rel="noopener noreferrer"> <i
                            class="fa fa-desktop"></i> OpenReview </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="quesada-22-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="quesada-22-abstract"></a>
        </div>
    </article>
    </div>


    <div id="quesada-22-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div class="image padded_block">
                    <img src="./assets/mtneuro.png" alt="">
                </div>
                <div>
                    <b>Abstract:</b> There are multiple scales of abstraction from which we can describe the same image,
                    depending on whether we are focusing on fine-grained details or a more global attribute of the
                    image. In brain mapping, learning to automatically parse images to build representations of both
                    small-scale features (e.g., the presence of cells or blood vessels) and global properties of an
                    image (e.g., source brain region) is a crucial and open challenge. However, most existing datasets
                    and benchmarks for neuroanatomy consider only a single downstream task at a time. We introduce a new
                    dataset, annotations, and multiple downstream tasks that provide diverse ways to readout information
                    about brain structure and architecture from the same image. Our multi-task neuroimaging benchmark
                    (MTNeuro) is built on volumetric, micrometer-resolution X-ray microtomography imaging of a large
                    thalamocortical section of mouse brain, encompassing multiple cortical and subcortical regions, that
                    reveals dense reconstructions of the underlying microstructure (i.e., cell bodies, vasculature, and
                    axons). We generated a number of different prediction challenges and evaluated several supervised
                    and self-supervised models for brain-region prediction and pixel-level semantic segmentation of
                    microstructures. Our experiments not only highlight the rich heterogeneity of this dataset, but also
                    provide insights into how self-supervised approaches can be used to learn representations that
                    capture multiple attributes of a single image and perform well on a variety of downstream tasks.
                    Datasets, code, and pre-trained baseline models are provided at: https://mtneuro.github.io/.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="quesada-22-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@article{quesadamtneuro,
  title={MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction},
  author={Quesada, Jorge and Sathidevi, Lakshmi and Liu, Ran and Ahad, Nauman and Jackson, Joy M and Azabou, Mehdi and Xiao, Jingyun and Liding, Chris and Urzay, Carolina and Gray-Roncal, William and Johnson, Erik Christopher, Dyer, Eva L.}
}</code></pre>
                APA
                <pre><code>Quesada, J., Sathidevi, L., Liu, R., Ahad, N., Jackson, J. M., Azabou, M., Xiao, J. and Liding, C. and Urzay, C. and Gray-Roncal, W. and Johnson, E. C. & Dyer, E. L. MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction.</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>



    <div class="block container bd-post-content entry" data-tags="behavior,ssl">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/bams.png" data-prompt="three mice in an arena, key points are drawn to track the mice and their trajectory, isometric view, 3d render">
            </p>
        </figure>
        <div class="media-content">

            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-stopwatch"></i>
                        Learning Behavior Representations Through Multi-Timescale <span class="nowrap">Bootstrapping
                </span>
                    </p>

                    <p class="tagp">
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-danger is-rounded is-light has-text-weight-bold">Behavior</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Natural behavior consists of dynamics that are
                                        both unpredictable, can switch suddenly, and unfold over many different timescales. We
                                        introduce BAMS which can capture <b>multi-timescale</b> factors that help in <b>behavioral
                                            analysis</b>. We apply BAMS on <b>mouse</b> and <b>legged robots</b> dataset.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="dropdown  is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Self-Supervised Learning</p>
                                </div>
                                <hr class="dropdown-divider">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We introduce a multi-scale representation
                                        learning model: we combine a pooling module that aggregates features extracted over
                                        encoders with <b>different temporal receptive fields</b>, and design a set of latent
                                        objectives to <b>bootstrap the representations in each respective space</b>.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    </p>

                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em><b>Mehdi Azabou</b>, Michael Mendelson, Maks Sorokin, Shantanu Thakoor, Nauman Ahad,
                        Carolina Urzay, Eva L Dyer</em></p>
                    <p class="conferencetitle">Conference on Vision and Pattern Recognition (CVPR), Workshop on Multi-Agent
                        Behavior (Oral), 2022</p>
                        </div>
                    <div id="bams-abstract" style="display: none;">
                        <b>Abstract:</b> Natural behavior consists of dynamics that are both unpredictable, can switch suddenly,
                        and unfold over many different timescales. While some success has been found in building representations
                        of behavior under constrained or simplified task-based conditions, many of these models cannot be
                        applied to free and naturalistic settings due to the fact that they assume a single scale of temporal
                        dynamics. In this work, we introduce Bootstrap Across Multiple Scales (BAMS), a multi-scale
                        representation learning model for behavior: we combine a pooling module that aggregates features
                        extracted over encoders with different temporal receptive fields, and design a set of latent objectives
                        to bootstrap the representations in each respective space to encourage disentanglement across different
                        timescales. We first apply our method on a dataset of quadrupeds navigating in different terrain types,
                        and show that our model captures the temporal complexity of behavior. We then apply our method to the
                        MABe 2022 Multi-agent behavior challenge, where our model ranks 3rd overall and 1st on two subtasks, and
                        show the importance of incorporating multi-timescales when analyzing behavior.
                    </div>
                    <div class="subtitle">
                    <hr>
                    <span class="nowrap"><a href="https://arxiv.org/pdf/2206.07041.pdf" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a> &nbsp;</span>
                    <span class="nowrap"><a href="./assets/bams_poster.pdf" target="_blank" rel="noopener noreferrer"> <i
                            class="fa-solid fa-person-chalkboard"></i> Poster </a> &nbsp;</span>
                    <span class="nowrap"><a href="./assets/bams_slides.pdf" target="_blank" rel="noopener noreferrer"> <i
                            class="fa-solid fa-person-chalkboard"></i> Slides </a> &nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/abs/2206.07041" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="bams-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>

                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="bams-abstract"></a>
        </div>
    </article>
    </div>


    <div id="bams-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <b>Abstract:</b> Natural behavior consists of dynamics that are both unpredictable, can switch suddenly,
                and unfold over many different timescales. While some success has been found in building representations
                of behavior under constrained or simplified task-based conditions, many of these models cannot be
                applied to free and naturalistic settings due to the fact that they assume a single scale of temporal
                dynamics. In this work, we introduce Bootstrap Across Multiple Scales (BAMS), a multi-scale
                representation learning model for behavior: we combine a pooling module that aggregates features
                extracted over encoders with different temporal receptive fields, and design a set of latent objectives
                to bootstrap the representations in each respective space to encourage disentanglement across different
                timescales. We first apply our method on a dataset of quadrupeds navigating in different terrain types,
                and show that our model captures the temporal complexity of behavior. We then apply our method to the
                MABe 2022 Multi-agent behavior challenge, where our model ranks 3rd overall and 1st on two subtasks, and
                show the importance of incorporating multi-timescales when analyzing behavior.
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="bams-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@article{azabou2022learning,
  title={Learning Behavior Representations Through Multi-Timescale Bootstrapping},
  author={Azabou, Mehdi and Mendelson, Michael and Sorokin, Maks and Thakoor, Shantanu and Ahad, Nauman and Urzay, Carolina and Dyer, Eva L},
  journal={arXiv preprint arXiv:2206.07041},
  year={2022}
}</code></pre>
                APA
                <pre><code>Azabou, M., Mendelson, M., Sorokin, M., Thakoor, S., Ahad, N., Urzay, C., & Dyer, E. L. (2022). Learning Behavior Representations Through Multi-Timescale Bootstrapping. <i>arXiv preprint</i> arXiv:2206.07041.</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>


    <div class="block container bd-post-content entry year" data-tags="header">
        <h2 class="title">
            <p class="is-size-66">
                2021
            </p>
        </h2>
    </div>

    <div class="block container bd-post-content entry" data-tags="neuro,ssl">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/swapvae.png" data-prompt="3d arrows interlacing in the center, with pointy arrows, render, colorful, space background">
            </p>
        </figure>
        <div class="media-content">

            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-shuffle"></i>
                        Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural <span class="nowrap">Activity
                </span>
                    </p>
                    <p class="tagp">
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-success is-rounded is-light has-text-weight-bold">Neuroscience</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Through evaluations on both synthetic data
                                        and neural recordings from hundreds of neurons in different primate brains,
                                        we show that it is possible to build representations that <b>disentangle</b> neural
                                        datasets along <b>relevant latent dimensions linked to behavior</b>.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="dropdown  is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Self-Supervised Learning</p>
                                </div>
                                <hr class="dropdown-divider">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7"> Views are created by <b>dropping out neurons</b>
                                        and <b>jittering samples in time</b>, which intuitively should lead the network to a
                                        representation that maintains both temporal consistency and invariance
                                        to the specific neurons used to represent the neural state.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Generative Modeling</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Our approach combines a generative modeling framework with an instance-specific alignment loss that tries to maximize the representational similarity between transformed views of the input.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em>Ran Liu, <b>Mehdi Azabou</b>, Max Dabagia, Chi-Heng Lin, Mohammad Gheshlaghi Azar,
                        Keith B. Hengen,
                        Michal Valko, Eva L. Dyer</em></p>
                    <p class="conferencetitle">Neural Information Processing Systems (NeurIPS), accepted for Oral (1%
                        submissions), 2021</p>
                        </div>
                        <div id="swap-vae-abstract" style="display: none;">
                            <b>Abstract:</b> Meaningful and simplified representations of neural activity can yield insights
                            into how and what information is being processed within a neural circuit. However, without labels,
                            finding representations that reveal the link between the brain and behavior can be challenging.
                            Here, we introduce a novel unsupervised approach for learning disentangled representations of neural
                            activity called SwapVAE. Our approach combines a generative modeling framework with an
                            instance-specific alignment loss that tries to maximize the representational similarity between
                            transformed views of the input (brain state). These transformed (or augmented) views are created by
                            dropping out neurons and jittering samples in time, which intuitively should lead the network to a
                            representation that maintains both temporal consistency and invariance to the specific neurons used
                            to represent the neural state. Through evaluations on both synthetic data and neural recordings from
                            hundreds of neurons in different primate brains, we show that it is possible to build
                            representations that disentangle neural datasets along relevant latent dimensions linked to
                            behavior.
                        </div>
                    <div class="subtitle">    
                    <hr>
                    <span class="nowrap"><a href="https://nerdslab.github.io/SwapVAE/" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-book"></i> Project page </a>&nbsp;</span>
                    <span class="nowrap"><a
                            href="https://papers.nips.cc/paper/2021/file/58182b82110146887c02dbd78719e3d5-Paper.pdf"
                            target="_blank" rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://github.com/nerdslab/SwapVAE" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-github"></i> Code </a>&nbsp;</span>
                    <span class="nowrap"><a
                            href="https://slideslive.com/38968190/drop-swap-and-generate-a-selfsupervised-approach-for-generating-neural-activity?ref=speaker-25659"
                            target="_blank" rel="noopener noreferrer"> <i class="fa-solid fa-person-chalkboard"></i> Talk </a>&nbsp;</span>
                    <span class="nowrap"><a
                            href="https://papers.nips.cc/paper/2021/hash/58182b82110146887c02dbd78719e3d5-Abstract.html"
                            target="_blank" rel="noopener noreferrer"> <i class="fa fa-desktop"></i> NeurIPS Proceedings </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://openreview.net/forum?id=ZRPRjfAF3yd" target="_blank"
                                            rel="noopener noreferrer"> <i
                            class="fa fa-desktop"></i> OpenReview </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/abs/2111.02338" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="swap-vae-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="swap-vae-abstract"></a>
        </div>
    </article>
    </div>


    <div id="swap-vae-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div class="image padded_block">
                    <img src="./assets/overview_swapVAE.jpg" alt="">
                </div>
                <div>
                    <b>Abstract:</b> Meaningful and simplified representations of neural activity can yield insights
                    into how and what information is being processed within a neural circuit. However, without labels,
                    finding representations that reveal the link between the brain and behavior can be challenging.
                    Here, we introduce a novel unsupervised approach for learning disentangled representations of neural
                    activity called SwapVAE. Our approach combines a generative modeling framework with an
                    instance-specific alignment loss that tries to maximize the representational similarity between
                    transformed views of the input (brain state). These transformed (or augmented) views are created by
                    dropping out neurons and jittering samples in time, which intuitively should lead the network to a
                    representation that maintains both temporal consistency and invariance to the specific neurons used
                    to represent the neural state. Through evaluations on both synthetic data and neural recordings from
                    hundreds of neurons in different primate brains, we show that it is possible to build
                    representations that disentangle neural datasets along relevant latent dimensions linked to
                    behavior.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="swap-vae-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@inproceedings{NEURIPS2021_58182b82,
 author = {Liu, Ran and Azabou, Mehdi and Dabagia, Max and Lin, Chi-Heng and Gheshlaghi Azar, Mohammad and Hengen, Keith and Valko, Michal and Dyer, Eva},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10587--10599},
 publisher = {Curran Associates, Inc.},
 title = {Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural Activity},
 url = {https://proceedings.neurips.cc/paper/2021/file/58182b82110146887c02dbd78719e3d5-Paper.pdf},
 volume = {34},
 year = {2021}
}</code></pre>
                APA
                <pre><code>Liu, R., Azabou, M., Dabagia, M., Lin, C. H., Gheshlaghi Azar, M., Hengen, K., Valko, M. & Dyer, E. (2021). Drop, swap, and generate: A self-supervised approach for generating neural activity. <i>Advances in Neural Information Processing Systems, 34</i>, 10587-10599.</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div class="block container bd-post-content entry" data-tags="neuro,ssl">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/myow.png" data-prompt="robot cat, Sherlock holmes style, inspecting a stack of scattered images on a table, trending on artstation">

            </p>
        </figure>
        <div class="media-content">
            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-cat"></i>
                        Mine your own view: Self-supervised learning through across-sample <span class="nowrap">prediction
                </span>
                    </p>
                    <p class="tagp">

                    <div class="dropdown  is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Self-Supervised Learning</p>
                                </div>
                                <hr class="dropdown-divider">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">How can we introduce diverse views for learning?
                                        We propose the use of <b>the dataset itself</b> to find <b>similar, yet distinct, samples</b> to serve as views for one another.
                                        </p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Vision</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We benchmark MYOW against other SOTA SSL methods on CIFAR-10(0) and Tiny Imagenet.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-success is-rounded is-light has-text-weight-bold">Neuroscience</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">When tested on <b>multi-unit neural
                                        recordings</b> in rodents (V1, CA1) and primates (MC), we find that MYOW
                                        outperforms other self-supervised approaches in all examples (in some cases by more than 10%),
                                        and often <b>surpasses the supervised baseline</b>. </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em><b>Mehdi Azabou</b>, Mohammad Gheshlaghi Azar, Ran Liu, Chi-Heng Lin, Erik C.
                        Johnson,
                        Kiran Bhaskaran-Nair, Max Dabagia, Bernardo Avila-Pires, Lindsey Kitchell,
                        Keith B. Hengen, William Gray-Roncal, Michal Valko, Eva L. Dyer</em></p>
                    <p class="conferencetitle">Neural Information Processing Systems (NeurIPS), Workshop on Self-supervised
                        Learning: Theory and Practice (Oral), Feb 2021</p>
                    </div>
                    <div id="myow-abstract" style="display: none;">
                        <b>Abstract:</b> State-of-the-art methods for self-supervised learning (SSL) build representations
                        by maximizing the similarity between different transformed "views" of a sample. Without sufficient
                        diversity in the transformations used to create views, however, it can be difficult to overcome
                        nuisance variables in the data and build rich representations. This motivates the use of the dataset
                        itself to find similar, yet distinct, samples to serve as views for one another. In this paper, we
                        introduce Mine Your Own vieW (MYOW), a new approach for self-supervised learning that looks within
                        the dataset to define diverse targets for prediction. The idea behind our approach is to actively
                        mine views, finding samples that are neighbors in the representation space of the network, and then
                        predict, from one sample's latent representation, the representation of a nearby sample. After
                        showing the promise of MYOW on benchmarks used in computer vision, we highlight the power of this
                        idea in a novel application in neuroscience where SSL has yet to be applied. When tested on
                        multi-unit neural recordings, we find that MYOW outperforms other self-supervised approaches in all
                        examples (in some cases by more than 10%), and often surpasses the supervised baseline. With MYOW,
                        we show that it is possible to harness the diversity of the data to build rich views and leverage
                        self-supervision in new domains where augmentations are limited or unknown.
                    </div>
                    <div class="subtitle">
                        <hr>
                    <span class="nowrap"><a href="https://nerdslab.github.io/myow/" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-book"></i> Project page </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/pdf/2102.10106.pdf" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://github.com/nerdslab/myow" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-github"></i> Code </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://sslneurips21.github.io/files/Poster/poster%2038.pdf" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa-solid fa-person-chalkboard"></i> Poster </a>&nbsp;</span>
                    <span class="nowrap"><a
                            href="https://slideslive.com/38972685/mine-your-own-view-a-selfsupervised-approach-for-learning-representations-of-neural-activity?ref=recommended"
                            target="_blank" rel="noopener noreferrer"> <i class="fa-solid fa-person-chalkboard"></i> Talk </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/abs/2102.10106" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="myow-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="myow-abstract"></a>
        </div>
    </article>
    </div>

    <div id="myow-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div class="image padded_block">
                    <img src="./assets/myow_mined.gif" alt="">
                </div>
                <div>
                    <b>Abstract:</b> State-of-the-art methods for self-supervised learning (SSL) build representations
                    by maximizing the similarity between different transformed "views" of a sample. Without sufficient
                    diversity in the transformations used to create views, however, it can be difficult to overcome
                    nuisance variables in the data and build rich representations. This motivates the use of the dataset
                    itself to find similar, yet distinct, samples to serve as views for one another. In this paper, we
                    introduce Mine Your Own vieW (MYOW), a new approach for self-supervised learning that looks within
                    the dataset to define diverse targets for prediction. The idea behind our approach is to actively
                    mine views, finding samples that are neighbors in the representation space of the network, and then
                    predict, from one sample's latent representation, the representation of a nearby sample. After
                    showing the promise of MYOW on benchmarks used in computer vision, we highlight the power of this
                    idea in a novel application in neuroscience where SSL has yet to be applied. When tested on
                    multi-unit neural recordings, we find that MYOW outperforms other self-supervised approaches in all
                    examples (in some cases by more than 10%), and often surpasses the supervised baseline. With MYOW,
                    we show that it is possible to harness the diversity of the data to build rich views and leverage
                    self-supervision in new domains where augmentations are limited or unknown.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="myow-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@article{azabou2021mine,
  title={Mine your own view: Self-supervised learning through across-sample prediction},
  author={Azabou, Mehdi and Azar, Mohammad Gheshlaghi and Liu, Ran and Lin, Chi-Heng and Johnson, Erik C and Bhaskaran-Nair, Kiran and Dabagia, Max and Avila-Pires, Bernardo and Kitchell, Lindsey and Hengen, Keith B and Gray-Roncal, William and Valko, Michal and Dyer, Eva L},
  journal={arXiv preprint arXiv:2102.10106},
  year={2021}
  }</code></pre>
                APA
                <pre><code>Azabou, M., Azar, M. G., Liu, R., Lin, C. H., Johnson, E. C., Bhaskaran-Nair, K., Dabagia, M., Avila-Pires, B., Kitchell, L., Hengen, K. B., Gray-Roncal, W., Valko, M. & Dyer, E. L. (2021). Mine your own view: Self-supervised learning through across-sample prediction. <i>arXiv preprint</i> arXiv:2102.10106.</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>




    <div class="block container bd-post-content entry" data-tags="neuro,ssl">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/neurossl.png" data-prompt="shadow art hyperrealistic colorful brain, studio photography, isometric view">
            </p>
        </figure>
        <div class="media-content">
            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-brain"></i>
                        Using self-supervision and augmentations to build insights into <span class="nowrap"> neural coding
                </span>
                    </p>
                    <p class="tagp">
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-success is-rounded is-light has-text-weight-bold">Neuroscience</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We investigate recent work in the
                                        application of self-supervised learning (SSL) for neuroscience, and discuss
                                    the types of data augmentations needed to develop the next <b>brain-machine interfaces</b>.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="dropdown  is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-info is-rounded is-light has-text-weight-bold">SSL</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Self-Supervised Learning</p>
                                </div>
                                <hr class="dropdown-divider">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">Many self-supervised methods are based on
                                        learning <b>invariances</b> across different views of the data. The <b>augmentations</b> that are used
                                        help define what these invariances are. We ask what is required to apply SSL in the
                                    neuroscience domain. </p>
                                </div>
                            </div>
                        </div>
                    </div>
                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em><b>Mehdi Azabou</b><sup>+</sup>, Max Dabagia<sup>+</sup>, Ran Liu<sup>+</sup>,
                        Chi-Heng Lin, Keith B. Hengen, Eva L. Dyer</em></p>
                    <p><em><sup>+</sup>Contributed equally as co-first authors</em></p>
                    <p class="conferencetitle">Neural Information Processing Systems (NeurIPS), Workshop on Self-supervised
                        Learning: Theory and Practice, <span class="nowrap">Feb 2021</span></p>
                        </div>
                    <div id="neuro-ssl-abstract" style="display: none;">                
                        <b>Abstract:</b> Self-supervised learning (SSL) provides a powerful mechanism for building
                        representations of complex data without the need for labels. In this perspective piece, we highlight
                        recent progress in the application of self-supervised learning (SSL) to data analysis in neuroscience,
                        discuss the implications of these results, and suggest ways in which SSL might be applied to reveal
                        interesting properties of neural computation.
                    </div>
                    <div class="subtitle">
                        <hr>
                    <span class="nowrap"><a href="https://sslneurips21.github.io/files/CameraReady/neural_ssl_workshop.pdf"
                                            target="_blank" rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://sslneurips21.github.io/files/Poster/poster%2036.pdf" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa-solid fa-person-chalkboard"></i> Poster</a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="neuro-ssl-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>

                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="neuro-ssl-abstract"></a>
        </div>
    </article>
    </div>



    <div id="neuro-ssl-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <b>Abstract:</b> Self-supervised learning (SSL) provides a powerful mechanism for building
                representations of complex data without the need for labels. In this perspective piece, we highlight
                recent progress in the application of self-supervised learning (SSL) to data analysis in neuroscience,
                discuss the implications of these results, and suggest ways in which SSL might be applied to reveal
                interesting properties of neural computation.
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="neuro-ssl-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@article{azabouusing,
  title={Using self-supervision and augmentations to build insights into neural coding},
  author={Azabou, Mehdi and Dabagia, Max and Liu, Ran and Lin, Chi-Heng and Hengen, Keith B and Dyer, Eva L},
  journal={NeurIPS 2021 Workshop on Self-supervised Learning: Theory and Practice},
  year = {2021}
}</code></pre>
                APA
                <pre><code>Azabou, M., Dabagia, M., Liu, R., Lin, C. H., Hengen, K. B. & Dyer, E. L. Using self-supervision and augmentations to build insights into neural coding.</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <!-- LOT -->
    <div class="block container bd-post-content entry" data-tags="ot,da">
    <article class="media bottom_padding">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/lot.png" data-prompt="a complex network of highways made with octopi, digital art, high resolution">
            </p>
        </figure>
        <div class="media-content">
            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-anchor"></i>
                        Making transport more robust and interpretable by moving data through a small number of anchor <span
                            class="nowrap">points</span>
                    </p>

                    <p class="tagp">

                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Optimal Transport</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">By imposing a <b>low rank</b> structure to the
                                        transport plan, we show that our approach is more <b>robust</b> to outliers and
                                        in high-dimensional spaces compared to vanilla OT and other approaches.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-light has-text-weight-bold">Domain Adaptation</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">We apply LOT to different domain
                                        adaptation problems, and show how our approach is both effective and
                                        <b>interpretable</b>.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em>Chi-Heng Lin, <b>Mehdi Azabou</b>, Eva L. Dyer</em></p>
                    <p class="conferencetitle">International Conference on Machine Learning (ICML), 2021</p>
                    </div>
                    <div id="lin-20-abstract" style="display: none;">
                        <b>Abstract:</b> Optimal transport (OT) is a widely used technique for distribution alignment, with
                        applications throughout the machine learning, graphics, and vision communities. Without any
                        additional structural assumptions on transport, however, OT can be fragile to outliers or noise,
                        especially in high dimensions. Here, we introduce Latent OptimalTransport (LOT), a new approach for
                        OT that simultaneously learns low-dimensional structure in data while leveraging this structure to
                        solve the alignment task. The idea behind our approach is to learn two sets of ‚Äúanchors‚Äù that
                        constrain the flow of transport between a source and target distribution. In both theoretical and
                        empirical studies, we show thatLOTregularizes the rank of transport and makes it more robust to
                        outliers and the sampling density. We show that by allowing the source and target to have different
                        anchors, and using LOT to align the latent spaces between anchors, the resulting transport plan has
                        better structural interpretability and highlights connections between both the individual data
                        points and the local geometry of the datasets.
                    </div>
                    <div class="subtitle">
                    <hr>
                    <span class="nowrap"><a href="https://nerdslab.github.io/latentOT/" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-book"></i> Project page </a>&nbsp;</span>
                    <span class="nowrap"><a href="http://proceedings.mlr.press/v139/lin21a/lin21a.pdf" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://github.com/nerdslab/latentOT" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-github"></i> Code </a>&nbsp;</span>
                    <span class="nowrap"><a
                            href="https://cpn-us-w2.wpmucdn.com/sites.gatech.edu/dist/9/630/files/2021/07/icml_lot_final.pdf"
                            target="_blank" rel="noopener noreferrer"> <i class="fa-solid fa-person-chalkboard"></i> Poster </a>&nbsp;</span>
                    <span class="nowrap"><a href="http://proceedings.mlr.press/v139/lin21a.html" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-desktop"></i> ICML Proceedings </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://arxiv.org/abs/2102.10106" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-desktop"></i> arXiv </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="lot-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>

                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="lin-20-abstract"></a>
        </div>
    </article>
    </div>


    <div id="lin-20-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <div class="image padded_block">
                    <img src="./assets/lot.png" alt="">
                </div>
                <div>
                    <b>Abstract:</b> Optimal transport (OT) is a widely used technique for distribution alignment, with
                    applications throughout the machine learning, graphics, and vision communities. Without any
                    additional structural assumptions on transport, however, OT can be fragile to outliers or noise,
                    especially in high dimensions. Here, we introduce Latent OptimalTransport (LOT), a new approach for
                    OT that simultaneously learns low-dimensional structure in data while leveraging this structure to
                    solve the alignment task. The idea behind our approach is to learn two sets of ‚Äúanchors‚Äù that
                    constrain the flow of transport between a source and target distribution. In both theoretical and
                    empirical studies, we show thatLOTregularizes the rank of transport and makes it more robust to
                    outliers and the sampling density. We show that by allowing the source and target to have different
                    anchors, and using LOT to align the latent spaces between anchors, the resulting transport plan has
                    better structural interpretability and highlights connections between both the individual data
                    points and the local geometry of the datasets.
                </div>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="lot-cite" class="modal">

        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@InProceedings{lin2021,
  title = 	 {Making transport more robust and interpretable by moving data through a small number of anchor points},
  author =       {Lin, Chi-Heng and Azabou, Mehdi and Dyer, Eva},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6631--6641},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
}</code></pre>
                APA
                <pre><code>Lin, C., Azabou, M. &amp; Dyer, E.. (2021). Making transport more robust and interpretable by moving data through a small number of anchor points. <i>Proceedings of the 38th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 139:6631-6641 Available from https://proceedings.mlr.press/v139/lin21a.html.</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>


    <div class="block container bd-post-content entry year" data-tags="header">
        <h2 class="title">
            <p class="is-size-66">
                Abstracts
            </p>
        </h2>
    </div>

    <div class="block container bd-post-content entry" data-tags="neuro">
    <article class="media">
        <figure class="media-left is-hidden-mobile">
            <p class="image is-128x128">
                <img src="covers/changepoint.png" data-prompt="population of neurons morphing from one color to the other from left to right with a clear boundary, digital art">
            </p>
        </figure>
        <div class="media-content">
            <div class="block container bd-post-content">
                <h1 class="title">
                    <p class="is-size-55">
                        <i class="fa-solid fa-road-spikes"></i>
                        Detecting change points in neural population activity with contrastive metric <span class="nowrap">learning
                </span>
                    </p>

                    <p class="tagp">

                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-success is-rounded is-light has-text-weight-bold">Neuroscience</span>
                    </span>
                        </div>
                        <div class="dropdown-menu" role="menu">
                            <div class="dropdown-content">
                                <div class="dropdown-item">
                                    <p class="has-text-weight-normal is-size-7">
                                        We introduce an approach for identifying <b>switches</b> in neural population during
                                        <b>free behavior</b>, and find that different groups of neurons are useful for different contexts. </p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="dropdown is-hoverable">
                        <div class="dropdown-trigger">
                    <span class="ghostbutton">
                        <span class="tag is-rounded is-info is-light has-text-weight-bold">Abstract</span>
                    </span>
                        </div>
                    </div>


                    </p>
                </h1>
                <div class="subtitle subtitle-no-margin">
                    <p class="authors"><em>Carolina Urzay, Nauman Ahad, <b>Mehdi Azabou</b>, Aidan Schneider, Geethika Atmakuri,
                        Keith B. Hengen, Eva L. Dyer</em></p>
                    <p class="conferencetitle">Conference on Cognitive Computational Neuroscience (CCN), 2022</p>
                    </div>
                    <div id="urzay-221-abstract" style="display: none;">                
                        <b>Abstract:</b> Finding points in time where the distribution of neural responses changes (change
                        points) is an important step in many neural data analysis pipelines. However, in complex and free
                        behaviors, where we see different types of shifts occurring at different rates, it can be difficult to
                        use existing methods for change point (CP) detection because they can't necessarily handle different
                        types of changes that may occur in the underlying neural distribution. In this work, we introduce a new
                        approach for finding changes in neural population states across diverse activities and arousal states
                        occurring in free behavior. Our model follows a contrastive learning approach: we learn a metric for CP
                        detection based on maximizing the Sinkhorn divergences of neuron firing rates across two sides of a
                        labeled CP. We apply this method to a 12-hour neural recording of a freely behaving mouse to detect
                        changes in sleep stages and behavior. We show that when we learn a metric, we can better detect change
                        points and also yield insights into which neurons and sub-groups are important for detecting certain
                        types of switches that occur in the brain.
                    </div>
                    <div class="subtitle">
                    <hr>
                    <span class="nowrap"><a href="https://2022.ccneuro.org/proceedings/0000046.pdf" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-file-pdf"></i> Paper </a>&nbsp;</span>
                    <span class="nowrap"><a href="https://2022.ccneuro.org/view_paper.php?PaperNum=1261" target="_blank"
                                            rel="noopener noreferrer"> <i class="fa fa-desktop"></i> CCN Proceedings </a>&nbsp;</span>
                    <span class="nowrap"><a href="./assets/ccn_2022_poster.pdf" target="_blank" rel="noopener noreferrer"> <i
                            class="fa-solid fa-person-chalkboard"></i> Poster </a>&nbsp;</span>
                    <span class="nowrap"><a class="js-modal-trigger" data-target="urzay-22-cite"> <i
                            class="fa-solid fa-quote-right"></i> Cite</a></span>
                </div>
            </div>
        </div>
        <div class="media-right">
            <a class="js-expand-trigger fa-solid fa-chevron-down" data-target="urzay-221-abstract"></a>
        </div>
    </article>
    </div>

    <div id="urzay-22-modal" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title"></p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                <b>Abstract:</b> Finding points in time where the distribution of neural responses changes (change
                points) is an important step in many neural data analysis pipelines. However, in complex and free
                behaviors, where we see different types of shifts occurring at different rates, it can be difficult to
                use existing methods for change point (CP) detection because they can't necessarily handle different
                types of changes that may occur in the underlying neural distribution. In this work, we introduce a new
                approach for finding changes in neural population states across diverse activities and arousal states
                occurring in free behavior. Our model follows a contrastive learning approach: we learn a metric for CP
                detection based on maximizing the Sinkhorn divergences of neuron firing rates across two sides of a
                labeled CP. We apply this method to a 12-hour neural recording of a freely behaving mouse to detect
                changes in sleep stages and behavior. We show that when we learn a metric, we can better detect change
                points and also yield insights into which neurons and sub-groups are important for detecting certain
                types of switches that occur in the brain.
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>

    <div id="urzay-22-cite" class="modal">
        <div class="modal-background"></div>
        <div class="modal-card">
            <header class="modal-card-head">
                <p class="modal-card-title">Cite this paper</p>
                <button class="delete" aria-label="close"></button>
            </header>
            <section class="modal-card-body">
                BibTex
                <pre><code>@misc{Urzay_Ahad_Azabou_Schneider_Atmakuri_Hengen_Dyer_2022,
  title={Detecting change points in neural population activity with contrastive metric learning},
  url={http://dx.doi.org/10.32470/CCN.2022.1261-0},
  DOI={10.32470/ccn.2022.1261-0},
  journal={2022 Conference on Cognitive Computational Neuroscience},
  publisher={Cognitive Computational Neuroscience},
  author={Urzay, Carolina and Ahad, Nauman and Azabou, Mehdi and Schneider, Aidan and Atmakuri, Geethika and Hengen, Keith B. and Dyer, Eva L.},
  year={2022}
  }</code></pre>
                APA
                <pre><code>Urzay, C., Ahad, N., Azabou, M., Schneider, A., Atmakuri, G., Hengen, K. B., & Dyer, E. L. (2022). Detecting change points in neural population activity with contrastive metric learning. In <i>2022 Conference on Cognitive Computational Neuroscience</i>. https://doi.org/10.32470/ccn.2022.1261-0</code></pre>
            </section>
            <footer class="modal-card-foot"></footer>
        </div>
    </div>
</section>

<div></div>

<footer class="footer">
    <div class="section content">
        <div style="font-weight: 400; font-size: .6em;">VERSION</div>
        <div style="font-size: 1em;">2023 ¬© Edition 1.2</div>
<!--        <div style="max-width: 550px; margin: auto;">Feel free to copy <a href="https://github.com/mazabou/mehai">this website</a>. Some of the images have been generated using DALL¬∑E, an artificial intelligence tool developed by OpenAI.</div>-->

    </div>
</footer>


<script src="search.js"></script>
<script src="hover.js"></script>

<script>
    (async () => {
    const canvas = document.getElementById("my-canvas");

    canvas.confetti =
        canvas.confetti || (await confetti.create(canvas, { resize: false }));

  const duration = 3 * 1000,
  animationEnd = Date.now() + duration,
  defaults = { startVelocity: 10, spread: 360, ticks: 60, zIndex: 0, decay: 0.94, scalar:1.3, };

function randomInRange(min, max) {
  return Math.random() * (max - min) + min;
}

const interval = setInterval(function() {
  const timeLeft = animationEnd - Date.now();

  if (timeLeft <= 0) {
    return clearInterval(interval);
  }

  const particleCount = 20 * (timeLeft / duration);
  const particleCount2 = 10; 

  // since particles fall down, start a bit higher than random
  canvas.confetti(
    Object.assign({}, defaults, {
      particleCount,
      origin: { x: randomInRange(0.2, 0.6), y: Math.random() - 0.2 },
    })
  );
  canvas.confetti(
    Object.assign({}, 
        {   ...defaults, 
        particleCount: 3,
        scalar: 2.5,
        shapes: ["text"],
        shapeOptions: {
        text: {
            value: ["üê∞", "üèùÔ∏è"],
        }, }, },
    {
        particleCount2,
      origin: { x: randomInRange(0.2, 0.6), y: Math.random() - 0.2 },
    })
  );
  canvas.confetti(
    Object.assign({}, 
        {   ...defaults, 
        particleCount: 3,
        scalar: 1,
        shapes: ["text"],
        shapeOptions: {
        text: {
            value: ["üê∞", "üèùÔ∏è"],
        }, }, },
    {
        particleCount2,
      origin: { x: randomInRange(0.2, 0.6), y: Math.random() - 0.2 },
    })
  );

  canvas.confetti(
    Object.assign({}, defaults, {
      particleCount,
      origin: { x: randomInRange(0.7, 0.9), y: Math.random() - 0.2 },
    })
  );
  canvas.confetti(
    Object.assign({}, 
        {   ...defaults, 
        particleCount: 3,
        scalar: 2.5,
        shapes: ["text"],
        shapeOptions: {
        text: {
            value: ["üèùÔ∏è"],
        }, }, },
        {
        particleCount2,
      origin: { x: randomInRange(0.7, 0.9), y: Math.random() - 0.2 },
    })
  );
}, 250);

    // canvas.confetti({
    //     spread: 10,
    //     origin: { y: 1.2 },
    // });
    })();
</script>
</body>
</html>
